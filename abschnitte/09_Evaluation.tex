\section{Evaluation}

Nachdem nun eine prototypsche Implementierung beider Ansätze vorgenommen wurde, soll versucht werden, eine objektive und vergleichende Evaluierung durchzuführen.

Dazu muss zunächst eine passende Methodik der Evaluation definiert werden. Anschließend kann die Evaluation dann durchgeführt werden.

\subsection{Methodik der Evaluation}

Grundsätzlich meint Evaluation den "`Prozess der Beurteilung des Wertes eines Produktes, Prozesses oder eines Programmes, was nicht notwendigerweise systematische Verfahren oder datengestützte Beweise zur Untermauerung einer Beurteilung erfordert"' \citep[S.][S. 9]{evaluationorig}.

Das im Folgenden verwendete systematische Verfahren soll darin bestehen, dass zunächst bestimmte messbare Kriterien definiert werden, zum Beispiel die Kosten der implementierten Lösung. Damit die einzelnen Kriterien beziehungsweise deren Messpunkte miteinander in Beziehung gebracht werden können, sind zwei Vorarbeiten notwendig:

Erstens müssen die Messpunkte der einzelnen Kriterien einer bestimmten Skala zugeordnet werden. Dazu soll hier der Einfachheit halber die ordinale Skale der natürlichen Zahlen von 1 bis 10 verwendet werden. Die 10 meint dabei immer das beste Ergebnis, die 1 das schlechteste. Zum Beispiel würde man die Kosten der teureren Implementierung mit 1 bewerten und die der günstigeren mit dem entsprechenden proportionalen Wert, also zum Beispiel 5, falls der günstigere Ansatz ungefähr die Hälfte kostet. Eine kostenlose Implementierung würde entsprechend dem Wert 10 der Skala zugeordnet werden.

Zweitens ist es notwendig, die jeweiligen unterschiedlichen Kriterien zu gewichten, da ein Kriterium in Hinblick auf die konkrete Problemstellung weniger wichtig sein kann als ein anderes. Dies soll jeweils über die Definition eines Faktors erfolgen, der sich ebenfalls im Bereich 1 bis 10 bewegt. 10 würde also bedeuten, dass ein Kriterium sehr wichtig ist. 1 hingegen bedeuet, dass ein Kriterium eine sehr geringe Rolle spielt.

Mit Hilfe dieses Verfahrens ist es möglich, am Ende einen konkrete numerische Bewertung der Lösung zu errechnen, in dem man den gewichteten Mittelwert über alle Kriterien berechnet. Das Ergebnis würde sich somit ebenfalls im Bereich von 1 bis 10 bewegen, wobei eine sehr gute Lösung eben die Bewertung 10 erhalten würde, eine sehr schlechte Lösung die Bewertung 1.

Es ist wichtig zu betonen, dass, auch wenn das Verfahren aufgrund der mathematischen Herangehensweise einen sehr objektiven Eindruck macht, die Bewertung der einzelnen Kriterien und ihre zusätzliche Gewichtung rein subjektiv in Bezug auf die konkrete Problemstellung und die Ausgangslage bei der Pixelhouse GmbH erfolgt. Somit ist auch die schließliche Bewertung zumindest in diesem Sinne subjektiv beziehungsweise willkürlich.

\subsection{Durchführung der Evaluation}

Es ist nun notwendig, den zuvor recht allgemein beschriebenen Ansatz der Evaluation mit konkreten Kriterien und deren Gewichtung zu versehen. Anschließend kann dann die Ermittlung der Messwerte und eine Diskussion des errechneten Bewertungsergebnisses erfolgen.

\subsubsection{Definition und Gewichtung der Evaluationskriterien}

In Bezug auf die Problemstellung und Zielsetzung der Arbeit sind für die Pixelhouse GmbH vor allem Kriterien wichtig, die die Ausführungsdauer der Tests beinflussen. Dazu werden im Folgenden drei Kriterien definiert.

Erstens spielt die Dauer für das Erzeugen der eigentlichen Maschinen eine entscheidene Rolle. So muss nach jeder Änderung am Programmcode oder bei Änderungen an der Konfiguration ein neues Maschinenabbild für VirtualBox beziehungsweise ein neues Container-Image für Docker erzeugt werden. Die Dauer für die Erzeugung lässt sich derart der Skala von 1 bis 10 zuordnen, dass die langsamere der beiden Erzeugungsvorgänge die Bewertung 1 erhält und die anderen einen entsprechend proportionalen Wert, wenn man davon ausgeht, dass eine Erzeugung, die gar keine Zeit kostet, mit 10 Punkte bewertet wird.

Als zweites Kriterium wird die Dauer für das Starten der ersten Umgebung herangezogen, wobei hier ebenfalls der längere Startvorgang mit 1 und ein sofortiges Starten mit 10 bewertet wird. Hier wird bewusst das Starten der ersten Umgebung gemessen, da es sein könnte, dass mit jeder weiteren Umgebgung, die auf dem gleichen Hardware-Host gestartet wird, der Startvorgang potentiell langsamer wird, da sich die einzelnen Umgebungen ja letzendlich die gleiche Hardwareresourcen teilen.

Der zuletzt angedeutete Aspekt, nämlich ein geändertes Laufzeitverhalten aufgrund von endlichen Resourcen auf einer Hardwaremaschine, soll nun ebenfalls durch ein Kriterium messbar gemacht werden. Wie bereits in der Einleitung erwähnt, ist laut Hamble und Farley bei der Parallelieserung von Tests die Gesamtausführungs aller Tests lediglich durch die Dauer des längsten Tests und die Größe des Hardware-Budgets, also durch die Menge der zur Verfügung stehenden Hardware begrenzt \citep[Vgl.][S. 310]{HumFar10}. In Bezug auf die beiden unterschiedliche Ansätze kann man somit sagen, dass eine Lösung, die weniger Resourcen verbraucht, besser ist als diejenige, die mehr Resourcen verbraucht, da man mit ihr auf der gleichen zur Verfügung stehenden Hardware mehr Testumgebungen und somit mehr Tests parallelisieren kann. Zur Messung dieser Eigenschaft wird deshalb untersucht, wieviele Umgebungen sich mit dem jeweiligen Ansatz auf einer konkreten Testhardware starten lassen. Die größere Anzahl wird dann wiederrum mit dem Wert 10 bewertet, die kleinere entsprechend proportional, wobei 1 für nur eine gestartete Umgebung steht. Es wäre noch spannender, sogar das tatsächliche Laufzeitverhalten einer gestarteten Umgebung in Bezug auf die zur Verfügung stehenden Hardware-Resourcen und der Anzahl an anderen Testumgebungen zu untersuchen. So kann es zwar sein, dass beide Ansätze gleich viele Umgebungen starten können, diese aber langsamer oder schneller Anwendungsanfragen abarbeiten. Erste Versuche, auf dem zur Verfügung stehenden Testsystem entsprechende Zahlen zu ermitteln, haben sich aber als sehr schwierig herausgestellt, da gerade im Grenzbereich (also zum Beispiel komplett ausgenutzter Hardwareresourcen) sehr heterogene und schwer reproduzierbare Werte entstehen. Im Rahmen dieser Evaluation wird eine solche Messung also nicht vorgenommen. Bei Interesse an diesem Thema bietet sich die Lektüre des IBM Research Reports "`An Updated Performance Comparison of Virtual Machines and Linux Containers"' an. In diesem wird die OS-Container-Lösung Docker und der Hypervisor KVM mit umfangreichen Tests auf deren Performance hin untersucht und verglichen. Dabei kommt der Bericht zu dem Ergebnis, dass Docker in jedem Test mindestens genauso performant ist wie KVM: "`Both VMs and containers are mature technology that have benefited from a decade of incremental hardware and software optimizations. In general, Docker equals or exceeds KVM performance in every case we tested."' \citep[S.][Conclusion]{comparison}

Alle drei genannten Kriterien, Erzeugungsdauer, Startdauer und Menge an Umgebungen pro konkreter Hardwareresource, sind für die Problemstellung und Ausgangssituation bei der Pixelhouse GmbH von großer Bedeutung. Da aber grundsätzlich ja eine größtmögliche Parallelisierung der Testausführung angestrebt wird, ist die Menge der Umgebungen das wichtigste Kriterium. Es wird somit mit dem Faktor 10 gewichtet. Da sich auch die Erzeugungsdauer der Maschinenabbilder nur konstant auf die Ausführungsdauer aller Tests auswirkt, wird sie lediglich mit dem Faktor 4 gewichtet. Auch die Startdauer einer einzelnen Umgebung spielt eine geringere Rolle, da sie, wenn man von einer maximalen Parallelisierung ausgehen würde, die Gesamtausführungsdauer aller Tests ebenfalls nur konstant erhöht und nicht zum Beispiel linear oder sogar exponentiell. Dauert das Starten zum Beispiel 10 Minuten, würde die Gesamtausführungsdauer aller Tests sich eben um 10 Minuten erhöhen, wenn man alle Tests parallel ausführt. Da es aber fraglich ist, ob die Pixelhouse GmbH wirklich eine vollständige Parallelisierung aller Tests erreichen wird beziehungsweise bezahlen möchte, wird dieses Kriterium etwas höher, nämlich mit dem Faktor 7 gewichtet.

Neben Kriterien, die Einfluss auf die Ausführungsdauer haben, spielen für die Pixelhouse nun lediglich noch Kriterien eine Rolle, die die Verwendbarkeit des jeweiligen Ansatzes kennzeichnen. Hier soll jeweils die bessere Lösung mit dem Wert 10 bewertet werden, die schlechtere Lösung mit 1. Die konkreten Kriterien sind dabei: Installationsaufwand, Einfachheit der Konfigurationssyntax (für Maschinen und Umgebungsdefinition) und Einfachheit der Umgebungssteuerung (Kommandozeilentools). Die Gewichtung sieht dabei so aus, dass der Installationsaufwand und die Einfacheit der Umgebungssteuerung lediglich mit dem Faktor 3 gewichtet wird. Diese Aspekte spielen im Alltag der Entwicklung keine entscheidene Rolle, da die Installation nicht regelmäßig oder sogar automatisiert erfolgt und die Umgebungssteuerung im Falle der automatischen Testausführung ebenfalls programmatisch erfolgt und nicht händisch erfolgt. Die Einfachheit Konfigurationssyntax kann gerade für die Administratoren regelmäßig von Vorteil sein. Sie wird deshalb mit dem Faktor 5 gewichtet. 

Die Kriterien und ihre Gewichtung werden in der Abbildung \ref{kriterien} noch einmal zusammengestellt.

\begin{figure}[!ht]
  \begin{center}
    \resizebox{15cm}{!} {
      \begin{tabular}{|l|r|}
      \hline
      Evaluationskriterium & Gewichtungsfaktor \\
      \hline
      Dauer der Erzeugung & 4 \\
      \hline
      Dauer des Startvorgangs & 7 \\
      \hline
      Menge der parallelen Testumgebungen & 10 \\
      \hline
      Einfachheit der Installation & 3 \\
      \hline
      Einfachheit der Umgebungssteuerung & 3 \\      
      \hline
      Einfachheit der Konfigurationssyntax & 5 \\
      \hline
      \end{tabular}
    }
    \caption{Zusammenfassung Evaluationskriterien und Gewichtung}
    \label{kriterien}
  \end{center}
\end{figure}

\subsubsection{Ermittlung und Diskussion der Ergebnisse}

Bei der Ermittlung der tatsächlich messbaren Kriterien ist zunächst interessant, auf welchem Testsystem die entsprechenden Zahlen ermittelt werden. Beim Testsystem handelt es sich um einen Laptop, der einen Intel Core i7 mit einer Taktung von 3 GHz und zwei Prozessorkernen enthält. Außerdem verfügt das System über 4 GB DDR3 Hauptspeicher mit einer Taktung von 1600 MHz. Außerdem verfügt das System über eine 120 Mbit Internetanbindung, was gerade beim Erzeugen der Maschinen-Abbilder beziehungsweise OS-Container-Images und der Installation von Softwarepaketen eine Rolle spielt. Bei der Festplatte handelt es sich um eine 256 GB großes \ac{SSD}.

Sämtliche Messergebnisse befinden sich sowohl auf der beiliegenden CD im Ordner Evaluation als auch online unter der Adresse:

\href{https://github.com/perprogramming/bachelor-arbeit/tree/master/cd/Evaluation}{https://github.com/perprogramming/bachelor-arbeit/tree/master/cd/Evaluation}.

Die Erzeugung aller Maschinen-Abbilder für VirtualBox mit Hilfe des Tools Packer hat auf diesem System ca. 25 Minuten gedauert. Das Protokoll der Erzeugung kann auf der beiliegenden CD in der Datei Evaluation/build\_virtualbox.log eingesehen werden. Die Erzeugung aller OS-Container-Images mit Docker hat auf dem gleichen Testsystem ca. 5 Minuten gedauert. Auch hier befindet sich das Protokoll auf der CD in der Datei Evaluation/build\_docker.log. Setzt man also bei der Bewertung der Erzeugungsdauer für die längere Dauer von ca. 25 Minuten im Falle von VirtualBox und Packer den Wert 1 an, so ergibt sich daraus für die Dauer von 5 Minuten eine ungefähre Bewertung von 8.

Auch der Startvorgang der ersten Umgebung wurde für beide Ansätze auf dem zuvor genannten Testsystem durchgeführt. Die entsprechenden Protokolle befinden sich ebenfalls auf der CD im Ordner Evaluation und heißen start\_virtualbox.log beziehungsweise start\_docker.log. Das Starten der ersten VirtualBox-Umgebung mit Hilfe von Vagrant dauerte auf dem Testsystem ca. 4 Minuten. Das Starten der ersten Docker-Umgebung dauerte auf dem gleichen System 2 Sekunden. Bei einem Unterschied dieser Größenordnung kann man für die Bewertung der Startdauer für die längere Zeitspanne von 4 Minuten den Wert 1 und für den schnelleren Vorgang den Wert 10 ansetzen.

Beim Versuch, auf dem Testsystem möglichst viele Umgebungen zu starten, stellte sich vor allem der verbrauchte Arbeitsspeicher als obere Grenze heraus. So war es mit VirtualBox beziehungsweise Vagrant möglich, insgesamt 3 Umgebungen zu starten. Da in jeder Maschine der Umgebung ein vollständiges Betriebssystem läuft, war während der Tests ein zuverlässiges Starten der Maschinen nur gewährleistet, wenn man über VirtualBox jeder Maschine ein Minimum von 256 MB Arbeitsspeicher zuweist. Pro Umgebung muss VirtualBox somit ungefähr 1,2 GB Arbeitsspeicher reservieren. Das Starten der vierten Umgebung war somit nicht mehr möglich. Eine Ansicht der 3 gleichzeitig laufenden Umgebungen befindet sich auf der CD unter Evaluation/number\_of\_environments\_virtualbox.log. Mit Docker war es auf dem oben beschriebenen Testsystem hingegen möglich insgesamt 13 Testumgebungen zu starten. Erst das Starten der 14 Umgebung endete mit der Fehlermeldung \textit{Untar fork/exec /usr/local/bin/docker: cannot allocate memory}. Da bei der Virtualisierung mittels OS-Container kein zusätzliches Betriebssystem gestartet wird, beschränkt sich der Arbeitsspeicherverbrauch auf den der tatsächlich in den Containern laufenden Anwendungen. Der Gesamtbedarf an Arbeitsspeicher für eine vollständige Docker-Umgebung ist dabei bei lediglich ca. 300 MB. Die Abbildung \ref{memory} stellt den Arbeitsspeicherverbrauch noch einmal im Vergleich dar. Eine Ansicht der 13 gleichzeitig laufenden Umgebungen ist auf der CD unter Evaluation/number\_of\_environments\_docker.log. Bei der Bewertung kann der Lösung mit Docker also die beste Bewertung von 10 zugeordnet werden. Die Implementierung mit VirtualBox erhält ungefähr proportional dazu die Bewertung 2.

\begin{figure}[!ht]
  \begin{center}
    \resizebox{15cm}{!} {
      \begin{tabular}{|l|r|r|}
      \hline
      Komponente & Docker-Container & VirtualBox VM \\
      \hline
      Loadbalancer & 100 MB & 256 MB \\
      \hline
      Appserver & 22 MB & 256 MB \\
      \hline
      Datenbankserver & 120 MB & 256 MB \\
      \hline
      Such-Index & 60 MB & 256 MB \\
      \hline
      Caches & 7 MB & 256 MB \\
      \hline
      Summe & 309 MB & 1280 MB \\
      \hline
      \end{tabular}
    }
    \caption{Arbeitsspeicherverbrauch einer Testumgebung}
    \label{memory}
  \end{center}
\end{figure}

Bei der Bewertung der Einfachheit der Installation kann kein signifikanter Unterschied festgestellt werden. Bei beiden Ansätzen müssen mehrere Anwendungsprogramme installiert werden. Es lässt sich lediglich feststellen, dass im Falle von Docker beide Anwendungen (docker und docker-compose) vom gleichen Hersteller, nämlich der Docker Inc., entwickelt werden. Im Falle von VirtualBox sind mehrere Firmen beziehungsweise Parteien beteiligt, nämlich Oracle für die Bereitstellung der Virtualisierung, die Firma HasiCorp für die Werkzeuge Packer und Vagrant und der Entwickler Adrien Thebo für das Vagrant-Plugin vagrant-hosts. Deshalb wird für dieses Kriterium eine Bewertung von 10 Punkten für Docker und eine Bewertung von 8 Punkten für die VirtualBox-Lösung vergeben.

Bei der Bewertung der Einfachheit der Bedienung lässt sich kein deutlicher Unterschied feststellen. Bei der Lösung für docker lässt sich das Bauen der Images über die Konfiguration der Gesamtumgebung anstoßen, so dass hier nur ein Aufruf von "`docker-compose build"' notwendig ist. docker-compose ruft dann seinerseits für jede Dockerfile das entsprechende "`docker build"' auf. Wie in der Abbildung \ref{buildvb} zu sehen, wird bei der Lösung für VirtualBox hierfür ein selbstgeschriebenes Script verwendet. Bei beiden Lösungen lässt sich die Umgebung über einen einfachen Aufruf starten. Docker erhält bei diesem Kriterium somit 10 Punkte, VirtualBox 8.

\begin{figure}[!ht]
  \begin{center}
    \externalcode{bash}{cd/Implementierung/build_virtualbox.sh}
    \caption{Build-Script für die VirtualBox-Maschinen}
    \label{buildvb}
  \end{center}
\end{figure}

Bei der Bewertung der Einfachheit der Konfigurationssyntax gibt es allerdings wieder einen deutlichen Unterschied. So können bei der Lösung mit Docker sehr einfache Konfigurationsdateien geschrieben werden, die im Schnitt deutlich kürzer sind als die für die Lösung mit VirtualBox. So sind für die Definition der Images der neuen Testumgebung mit Hilfe der Dockerfiles jeweils etwa 7 Zeilen notwendig. Bei der Definition der Maschinen-Abbild-Erzeugung für Packer sind hingegen jeweils ca. 32 Zeilen pro packer.json-Datei notwendig. Auch die Definition der Gesamtumgebung fällt bei docker-compose mit 21 Zeilen deutlich kürzer aus als die für Vagrant mit 74 Zeilen. Grundsätzlich handelt es sich bei der Datei docker-compose.yml außerdem um eine reine Konfigurationsdatei, wo hingegen die Datei Vagrantfile als ein Script verstanden werden muss, da es sich tatsächlich um ein kleines Programm in der Programmiersprache Ruby handelt. Dies hat zwar den Vorteil, dass man in der Vagrantfile durchaus Kontrollstrukturen wie zum Beispiel eine Schleife verwenden kann, um die Umgebung mit weniger Zeilen zu definieren. Dies bedeutet aber auch, dass die Anforderungen für die Erstellung und Pflege dieser Datei entsprechend höher sind als bei einer reinen Konfigurationsdatei. Insgesamt wird das Kriterium - also die Einfachheit der Konfigurationssyntax - mit 10 Punkten für docker  und 5 Punkten für den Ansatz mit VirtualBox bewertet.

In der Tabelle \ref{evaluationsergebniss} ist das daraus resultierende Gesamtergebnis inkl. des Berechneten gewichtetet Mittelwertes zu sehen.

Insgesamt errechnet sich für die Lösung mit Hilfe von VirtualBox eine Punktzahl von 3,4375 von 10 Punkten. Der Ansatz mit Docker erhält eine Punktzahl von 9,75 von 10 möglichen Punkten. Anhand dieser Bewertung lässt sich also sagen, dass eine Virtualisierung mit Hilfe von OS-Containern unter Berücksichtigung der gewählten Kriterien und der vorgenommenen Gewichtung deutlich besser eignet als eine Lösung mit dem Hosted-Hypervisor VirtualBox.

\begin{figure}[!ht]
  \begin{center}
    \resizebox{15cm}{!} {
      \begin{tabular}{|l|r|r|r|r|r|}
      \hline
      Evaluationskriterium & VirtualBox & Docker & Gewichtung & Summe VirtualBox & Summe Docker \\
      \hline
      Erzeugung & 1 & 8 & 4 & 4 & 32 \\
      \hline
      Startvorgang & 1 & 10 & 7 & 7 & 70 \\
      \hline
      Anzahl Testumgebungen & 2 & 10 & 10 & 20 & 100 \\
      \hline
      Installation & 8 & 10 & 3 & 24 & 30 \\
      \hline
      Umgebungssteuerung & 8 & 10 & 3 & 24 & 30 \\      
      \hline
      Konfigurationssyntax & 5 & 10 & 5 & 25 & 50 \\
      \hline
      Summe & & & 32 & 104 & 312 \\
      \hline
      Mittelwert & & & & 3,25 & 9,75 \\
      \hline
      \end{tabular}
    }
    \caption{Zusammenfassung Evaluationsergebnisse}
    \label{evaluationsergebniss}
  \end{center}
\end{figure}



