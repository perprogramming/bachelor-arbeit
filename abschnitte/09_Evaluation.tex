\section{Evaluation}

Nachdem nun eine prototypsche Implementierung beider Ansätze vorgenommen wurde, soll versucht werden, eine objektive und vergleichende Evaluierung durchzuführen.

Dazu muss zunächst eine passende Methodik der Evaluation definiert werden. Anschließend kann die Evaluation dann durchgeführt werden.

\subsection{Methodik der Evaluation}

Grundsätzlich meint Evaluation den "`Prozess der Beurteilung des Wertes eines Produktes, Prozesses oder eines Programmes, was nicht notwendigerweise systematische Verfahren oder datengestützte Beweise zur Untermauerung einer Beurteilung erfordert"' \citep[S.][S. 9]{evaluationorig}.

Das im Folgenden verwendete systematische Verfahren soll darin bestehen, dass zunächst bestimmte messbare Kriterien definiert werden, zum Beispiel die Kosten der implementierten Lösung. Damit die einzelnen Kriterien beziehungsweise deren Messpunkte miteinander in Beziehung gebracht werden können, sind zwei Vorarbeiten notwendig:

Erstens müssen die Messpunkte der einzelnen Kriterien einer bestimmten Skala zugeordnet werden. Dazu soll hier der Einfachheit halber die ordinale Skale der natürlichen Zahlen von 1 bis 10 verwendet werden. Die 10 meint dabei immer das beste Ergebnis, die 1 das schlechteste. Zum Beispiel würde man die Kosten der teureren Implementierung mit 1 bewerten und die der günstigeren mit dem entsprechenden proportionalen Wert, also zum Beispiel 5, falls der günstigere Ansatz ungefähr die Hälfte kostet. Eine kostenlose Implementierung würde entsprechend dem Wert 10 der Skala zugeordnet werden.

Zweitens ist es notwendig, die jeweiligen unterschiedlichen Kriterien zu gewichten, da ein Kriterium in Hinblick auf die konkrete Problemstellung weniger wichtig sein kann als ein anderes. Dies soll jeweils über die Definition eines Faktors erfolgen, der sich ebenfalls im Bereich 1 bis 10 bewegt. 10 würde also bedeuten, dass ein Kriterium sehr wichtig ist. 1 hingegen bedeuet, dass ein Kriterium eine sehr geringe Rolle spielt.

Mit Hilfe dieses Verfahrens ist es möglich, am Ende einen konkrete numerische Bewertung der Lösung zu errechnen, in dem man den gewichteten Mittelwert über alle Kriterien berechnet. Das Ergebnis würde sich somit ebenfalls im Bereich von 1 bis 10 bewegen, wobei eine sehr gute Lösung eben die Bewertung 10 erhalten würde, eine sehr schlechte Lösung die Bewertung 1.

Es ist wichtig zu betonen, dass, auch wenn das Verfahren aufgrund der mathematischen Herangehensweise einen sehr rationalen und objektiven Eindruck macht, die Gewichtung der einzelnen Kriterien rein subjektiv in Bezug auf die konkrete Problemstellung und die Ausgangslage bei der Pixelhouse GmbH erfolgt. Somit ist auch die schließliche Bewertung zumindest in diesem Sinne willkürlich.

\subsection{Durchführung der Evaluation}

Es ist nun notwendig, den zuvor recht allgemein beschriebenen Ansatz der Evaluation mit konkreten Kriterien und deren Gewichtung zu versehen. Anschließend kann dann die Ermittlung der Messwerte und eine Diskussion des errechneten Bewertungsergebnisses erfolgen.

\subsubsection{Definition und Gewichtung der Evaluationskriterien}

In Bezug auf die Problemstellung und Zielsetzung der Arbeit sind für die Pixelhouse GmbH vor allem Kriterien wichtig, die die Ausführungsdauer der Tests beinflussen. Dazu werden im Folgenden drei Kriterien definiert:

Erstens spielt die Dauer für das Erzeugen der eigentlichen Maschinen eine entscheidene Rolle. So muss nach jeder Änderung am Programmcode oder bei Änderungen an der Konfiguration ein neues Maschinenabbild für VirtualBox beziehungsweise ein neues Container-Image für Docker erzeugt werden. Die Dauer für die Erzeugung lässt sich derart der Skala von 1 bis 10 zuordnen, dass die langsamere der beiden Erzeugungsvorgänge die Bewertung 1 erhält und die anderen einen entsprechend proportionalen Wert, wenn man davon ausgeht, dass eine Erzeugung die gar keine Zeit kostet mit 10 Punkte bewertet wird.

Als zweites Kriterium wird die Dauer für das Starten der ersten Umgebung herangezogen, wobei hier ebenfalls der längere Startvorgang mit 1 und ein sofortiges Starten mit 10 bewertet wird. Hier wird bewusst das Starten der ersten Umgebung gemessen, da es sein könnte, dass mit jeder weiteren Umgebgung, die auf dem gleichen Hardware-Host gestartet wird, der Startvorgang potentiell langsamer wird, da sich die einzelnen Umgebungen ja letzendlich die gleiche Hardwareresourcen teilen.

Der zuletzt angedeutete Aspekt, nämlich ein geändertes Laufzeitverhalten aufgrund von endlichen Resourcen auf einer Hardwaremaschine, soll nun ebenfalls durch ein Kriterium messbar gemacht werden. Wie bereits in der Einleitung erwähnt, ist laut Hamble und Farley bei der Parallelieserung von Tests die Gesamtausführungs aller Tests lediglich durch die Dauer des längsten Tests und die Größe des Hardware-Budgets, also durch die Menge der zur Verfügung stehenden Hardware begrenzt \citep[Vgl.][S. 310]{HumFar10}. In Bezug auf die beiden unterschiedliche Ansätze kann man somit sagen, dass eine Lösung, die weniger Resourcen verbraucht, besser ist als diejenige, die mehr Resourcen verbraucht, da man mit ihr auf der gleichen zur Verfügung stehenden Hardware mehr Testumgebungen und somit mehr Tests parallelisieren kann. Zur Messung dieser Eigenschaft wird deshalb untersucht, wieviele Umgebungen sich mit dem jeweiligen Ansatz auf einer konkreten Testhardware starten lassen. Die größere Anzahl wird dann wiederrum mit dem Wert 10 bewertet, die kleinere entsprechend proportional, wobei 1 für nur eine gestartete Umgebung steht. Es wäre noch spannender, sogar das tatsächliche Laufzeitverhalten einer gestarteten Umgebung in Bezug auf die zur Verfügung stehenden Hardware-Resourcen und der Anzahl an anderen Testumgebungen zu untersuchen. So kann es zwar sein, dass beide Ansätze gleich viele Umgebungen starten können, diese aber langsamer oder schneller Anwendungsanfragen abarbeiten. Erste Versuche, entsprechende Zahlen zu ermitteln haben sich aber als sehr schwierig herausgestellt, da gerade im Grenzbereich (also zum Beispiel komplett ausgenutzter Hardwareresourcen) sehr heterogene und schwer reproduzierbare Werte entstehen. Im Rahmen dieser Evaluation wird eine solche Messung also nicht vorgenommen.

Alle drei genannten Kriterien, Erzeugungsdauer, Startdauer und Menge an Umgebungen pro konkreter Hardwareresource sind für die Problemstellung und Ausgangssituation bei der Pixelhouse GmbH von großer Bedeutung. Da aber grundsätzlich ja eine größtmögliche Parallelisierung der Testausführung angestrebt wird, ist die Menge der Umgebungen das wichtigste Kriterium. Es wird somit mit dem Faktor 10 gewichtet. Da sich auch die Erzeugungsdauer der Maschinenabbilder nur konstant auf die Ausführungsdauer aller Tests auswirkt, wird sie lediglich mit dem Faktor 4 gewichtet. Auch die Startdauer einer einzelnen Umgebung spielt eine geringere Rolle, da sie, wenn man von einer maximalen Parallelisierung ausgehen würde, die Gesamtausführungsdauer aller Tests ebenfalls nur konstant erhöht und nicht zum Beispiel linear oder sogar exponentiell. Dauert das Starten zum Beispiel 10 Minuten, würde die Gesamtausführungsdauer aller Tests sich eben um 10 Minuten erhöhen, wenn man alle Tests parallel ausführt. Da es aber fraglich ist, ob die Pixelhouse GmbH wirklich eine vollständige Parallelisierung aller Tests erreichen wird beziehungsweise bezahlen möchte, wird dieses Kriterium etwas höher, nämlich mit dem Faktor 7 gewichtet.

Neben Kriterien, die Einfluss auf die Ausführungsdauer haben, spielen für die Pixelhouse nun lediglich noch Kriterien eine Rolle, die die Verwendbarkeit des jeweiligen Ansatzes kennzeichnen. Hier soll jeweils die bessere Lösung mit dem Wert 10 bewertet werden, die schlechtere Lösung mit 1. Die konkreten Kriterien sind dabei: Installationsaufwand, Einfachheit der Konfigurationssyntax (für Maschinen und Umgebungsdefinition) und Einfachheit der Umgebungssteuerung (Kommandozeilentools). Die Gewichtung sieht dabei so aus, dass der Installationsaufwand und die Einfacheit der Umgebungssteuer lediglich mit dem Faktor 3 gewichtet wird. Diese Aspekte spielen im Alltag der Entwicklung keine entscheidene Rolle, da die Installation nicht regelmäßig oder sogar automatisiert erfolgt und die Umgebungssteuerung im Falle der automatischen Testausführung ebenfalls programmatisch erfolgt und nicht händisch erfolgt. Die Einfachheit Konfigurationssyntax kann gerade für die Administratoren regelmäßig von Vorteil sein. Sie wird deshalb mit dem Faktor 5 gewichtet. 

Die Kriterien und ihre Gewichtung werden in der Abbildung \ref{kriterien} noch einmal zusammengestellt.

\begin{figure}[!ht]
  \begin{center}
    \resizebox{15cm}{!} {
      \begin{tabular}{|l|r|}
      \hline
      Evaluationskriterium & Gewichtungsfaktor \\
      \hline
      Dauer der Erzeugung von Maschinen-Abbildern bzw. OS-Container-Images & 4 \\
      \hline
      Dauer des Startvorgangs der ersten Umgebung & 7 \\
      \hline
      Menge der parallelen Testumgebungen bei gegebener Hardware & 10 \\
      \hline
      Einfachheit der Installation der Virtualisierungslösung & 3 \\
      \hline
      Einfachheit der Umgebungssteuerung & 3 \\      
      \hline
      Einfachheit der Konfigurationssyntax & 5 \\
      \hline
      \end{tabular}
    }
    \caption{Zusammenfassung Evaluationskriterien und Gewichtung}
    \label{kriterien}
  \end{center}
\end{figure}

\subsubsection{Ermittlung und Diskussion der Ergebnisse}

Bei der Ermittlung der tatsächlich messbaren Kriterien ist zunächst interessant, auf welchem Testsystem die entsprechenden Zahlen ermittelt werden. Beim Testsystem handelt es sich um einen Laptop, der einen Intel Core i7 mit einer Taktung von 3 GHz und zwei Prozessorkernen enthält. Außerdem verfügt das System über 4 GB DDR3 Hauptspeicher mit einer Taktung von 1600 MHz. Außerdem verfügt das System über eine 120 Mbit Internetanbindung, was gerade beim Erzeugen der Maschinen-Abbilder beziehungsweise OS-Container-Images und der Installation von Softwarepaketen eine Rolle spielt. Bei der Festplatte handelt es sich um eine 256 GB großes \ac{SSD}.

Sämtliche Messergebnisse befinden sich sowohl auf der beiliegenden CD im Ordner Evaluation als auch unter https://github.com/perprogramming/bachelor-arbeit/tree/master/cd/Evaluation.

Die Erzeugung aller Maschinen-Abbilder für VirtualBox mit Hilfe des Tools Packer hat auf diesem System ca. 25 Minuten gedauert. Das Protokoll der Erzeugung kann auf der beiliegenden CD in der Datei Evaluation/build\_virtualbox.log eingesehen werden. Die Erzeugung aller OS-Container-Images mit Docker hat auf dem gleichen Testsystem ca. 5 Minuten gedauert. Auch hier befindet sich das Protokoll auf der CD in der Datei Evaluation/build\_docker.log. Setzt man also bei der Bewertung der Erzeugungsdauer für die längere Dauer von ca. 25 Minuten im Falle von Virtualbox und Packer den Wert 1 an, so ergibt sich daraus für die Dauer von 5 Minuten eine ungefähre Bewertung von 8.

Auch der Startvorgang der ersten Umgebung wurde für beide Ansätze auf dem zuvor genannten Testsystem durchgeführt. Die entsprechenden Protokolle befinden sich ebenfalls auf der CD im Ordner Evaluation und heißen start\_virtualbox.log beziehungsweise start\_docker.log. Das Starten der ersten Virtualbox-Umgebung mit Hilfe von Vagrant dauerte auf dem Testsystem ca. 4 Minuten. Das Starten der ersten Docker-Umgebung dauerte auf dem gleichen System 2 Sekunden. Bei einem Unterschied dieser Größenordnung kann man durchaus für die Bewertung der Startdauer für die längere Zeitspanne von 4 Minuten den Wert 1 und für den schnelleren Vorgang den Wert 10 ansetzen.

Beim Versuch, auf dem Testsystem möglichst viele Umgebungen zu starten, stellte sich vor allem der verbrauchte Arbeitsspeicher als obere Grenze heraus. So war es mit Virtualbox beziehungsweise Vagrant möglich, insgesamt 3 Umgebungen zu starten. Da in jeder Maschine der Umgebung ein vollständiges Betriebssystem läuft, war während der Tests ein zuverlässiges Starten der Maschinen nur gewährleistet, wenn man über Virtualbox jeder Maschine ein Minimum von 256 MB Arbeitsspeicher zuweist. Pro Umgebung muss Virtualbox somit ungefähr 1,2 GB Arbeitsspeicher reservieren. Das Starten der vierten Umgebung war somit nicht mehr möglich. Eine Ansicht der 3 gleichzeitig laufenden Umgebungen befindet sich auf der CD unter Evaluation/number\_of\_environments\_virtualbox.log. Mit Docker war es auf dem oben beschriebenen Testsystem hingegen möglich insgesamt 20 Testumgebungen zu starten. Erst das Starten der 21 Umgebung endete mit der Fehlermeldung \textit{Untar fork/exec /usr/local/bin/docker: cannot allocate memory}. Da bei der Virtualisierung mittels OS-Container kein zusätzliches Betriebssystem gestartet wird, beschränkt sich der Arbeitsspeicherverbrauch auf den der tatsächlich in den Containern laufenden Anwendungen. Der Gesamtbedarf an Arbeitsspeicher für eine vollständige Docker-Umgebung ist dabei bei lediglich ca. 200 MB. Eine Ansicht der 20 gleichzeitig laufenden Umgebungen ist auf der CD unter Evaluation/number\_of\_environments\_docker.log. Bei der Bewertung kann der Lösung mit Docker also die beste Bewertung von 10 zugeordnet werden. Die Implementierung mit Virtualbox erhält ungefähr proportional dazu die Bewertung 2.

Bei der Bewertung der Einfachheit der Installation kann kein signifikanter Unterschied festgestellt werden. Bei beiden Ansätzen müssen mehrere Anwendungsprogramme installiert werden. Es lässt sich lediglich feststellen, dass im Falle von Docker beide Anwendungen (docker und docker-compose) vom gleichen Hersteller, nämlich der Docker Inc. entwickelt werden. Im Falle von Virtualbox sind mehrere Firmen beteiligt, nämlich Oracle für die Bereitstellung der Virtualisierung, die Firma HasiCorp für die Werkzeuge Packer und Vagrant und der Entwickler Adrien Thebo für das Vagrant-Plugin vagrant-hosts. Deshalb wird für dieses Kriterium eine Bewertung von 10 Punkten für Docker und eine Bewertung von 8 Punkten für die Virtualbox-Lösung vergeben.

Bei der Bewertung der Einfachheit der Bedienung lässt sich kein Unterschied feststellen. Bei beiden Lösungen lässt sich die Erzeugung der Maschinen-Abbilder und die Steuerung der Umgebungen leicht über Kommandozeilen-Tools ansprechen. Beide Lösungen erhalten hier die volle Punktzahl von 10 Punkten.

Bei der Bewertung der Einfachheit der Konfigurationssyntax gibt es allerdings wieder einen deutlichen Unterschied. So können bei der Lösung mit Docker sehr einfache Konfigurationsdateien geschrieben werden, die im Schnitt deutlich kürzer sind als die für die Lösung mit Virtualbox. So sind für die Definition eines Images mit Hilfe einer Dockerfile durchschnittlich 7 Zeilen notwendig. Bei der Definition der Maschinen-Abbild-Erzeugung für Packer sind hingegen jeweils ca. 32 Zeilen pro packer.json-Datei notwendig. Auch die Definition der Gesamtumgebung fällt bei docker-compose mit 21 Zeilen deutlich kürzer aus als die für Vagrant mit 74 Zeilen. Grundsätzlich handelt es sich bei der Syntax in der docker-compose.yml auch um eine Deklaration, wo hingegen die Syntax der Datei Vagrantfile um die Programmiersprache Ruby handelt. Man könnte somit mit dem Hinzufügen von Logik sogar vermutlich eine kürzere Formulierung für die Vagrantfile ausarbeiten (zum Beispiel mit einer Schleife). Dies würde aber die Anforderungen für die Wartung dieser Datei entsprechend erhöhen. Insgesamt wird dieser Punkt also mit 10 Punkten für docker bewertet und mit nur 5 Punkten für den Ansatz mit Virtualbox.

In der Tabelle \ref{evaluationsergebniss} ist das daraus resultierende Gesamtergebnis inkl. des Berechneten gewichtetet Mittelwertes zu sehen.

\begin{figure}[!ht]
  \begin{center}
    \resizebox{15cm}{!} {
      \begin{tabular}{|l|r|r|r|r|r|}
      \hline
      Evaluationskriterium & Virtualbox & Docker & Gewichtung & Summe Virtualbox & Summe Docker \\
      \hline
      Dauer der Erzeugung & 1 & 8 & 4 & 4 & 32 \\
      \hline
      Dauer des Startvorgangs & 1 & 10 & 7 & 7 & 70 \\
      \hline
      Menge der parallelen Testumgebungen & 2 & 10 & 10 & 20 & 100 \\
      \hline
      Einfachheit der Installation & 8 & 10 & 3 & 24 & 30 \\
      \hline
      Einfachheit der Umgebungssteuerung & 10 & 10 & 3 & 30 & 30 \\      
      \hline
      Einfachheit der Konfigurationssyntax & 5 & 10 & 5 & 25 & 50 \\
      \hline
      Summe & & & 32 & 110 & 312 \\
      \hline
      Mittelwert & & & & 3,4375 & 9,75 \\
      \hline
      \end{tabular}
    }
    \caption{Zusammenfassung Evaluationsergebnisse}
    \label{evaluationsergebniss}
  \end{center}
\end{figure}



