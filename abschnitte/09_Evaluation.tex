\section{Evaluation}

Nachdem nun eine prototypsche Implementierung beider Ansätze vorgenommen wurde, soll versucht werden, eine objektive und vergleichende Evaluierung durchzuführen.

Dazu muss zunächst eine passende Methodik der Evaluation definiert werden. Anschließend kann die Evaluation dann durchgeführt werden.

\subsection{Methodik der Evaluation}

Grundsätzlich meint Evaluation den "`Prozess der Beurteilung des Wertes eines Produktes, Prozesses oder eines Programmes, was nicht notwendigerweise systematische Verfahren oder datengestützte Beweise zur Untermauerung einer Beurteilung erfordert"' \citep[S.][S. 9]{evaluationorig}.

Das im Folgenden verwendete systematische Verfahren soll darin bestehen, dass zunächst bestimmte messbare Kriterien definiert werden, zum Beispiel die Kosten der implementierten Lösung. Damit die einzelnen Kriterien beziehungsweise deren Messpunkte miteinander in Beziehung gebracht werden können, sind zwei Vorarbeiten notwendig:

Erstens müssen die Messpunkte der einzelnen Kriterien einer bestimmten Skala zugeordnet werden. Dazu soll hier der Einfachheit halber die ordinale Skale der natürlichen Zahlen von 1 bis 10 verwendet werden. Die 10 meint dabei immer das beste Ergebnis, die 1 das schlechteste. Zum Beispiel würde man die Kosten der teureren Implementierung mit 1 bewerten und die der günstigeren mit dem entsprechenden proportionalen Wert, also zum Beispiel 5, falls der günstigere Ansatz ungefähr die Hälfte kostet. Eine kostenlose Implementierung würde entsprechend dem Wert 10 der Skala zugeordnet werden.

Zweitens ist es notwendig, die jeweiligen unterschiedlichen Kriterien zu gewichten, da ein Kriterium in Hinblick auf die konkrete Problemstellung weniger wichtig sein kann als ein anderes. Dies soll jeweils über die Definition eines Faktors erfolgen, der sich ebenfalls im Bereich 1 bis 10 bewegt. 10 würde also bedeuten, dass ein Kriterium sehr wichtig ist. 1 hingegen bedeuet, dass ein Kriterium eine sehr geringe Rolle spielt.

Mit Hilfe dieses Verfahrens ist es möglich, am Ende einen konkrete numerische Bewertung der Lösung zu errechnen, in dem man den gewichteten Mittelwert über alle Kriterien berechnet. Das Ergebnis würde sich somit ebenfalls im Bereich von 1 bis 10 bewegen, wobei eine sehr gute Lösung eben die Bewertung 10 erhalten würde, eine sehr schlechte Lösung die Bewertung 1.

Es ist wichtig zu betonen, dass, auch wenn das Verfahren aufgrund der mathematischen Herangehensweise einen sehr rationalen und objektiven Eindruck macht, die Gewichtung der einzelnen Kriterien rein subjektiv in Bezug auf die konkrete Problemstellung und die Ausgangslage bei der Pixelhouse GmbH erfolgt. Somit ist auch die schließliche Bewertung zumindest in diesem Sinne willkürlich.

\subsection{Durchführung der Evaluation}

Es ist nun notwendig, den zuvor recht allgemein beschriebenen Ansatz der Evaluation mit konkreten Kriterien und deren Gewichtung zu versehen. Anschließend kann dann die Ermittlung der Messwerte und eine Diskussion des errechneten Bewertungsergebnisses erfolgen.

\subsubsection{Definition und Gewichtung der Evaluationskriterien}

In Bezug auf die Problemstellung und Zielsetzung der Arbeit sind für die Pixelhouse GmbH vor allem Kriterien wichtig, die die Ausführungsdauer der Tests beinflussen. Dazu werden im Folgenden drei Kriterien definiert:

Erstens spielt die Dauer für das Erzeugen der eigentlichen Maschinen eine entscheidene Rolle. So muss nach jeder Änderung am Programmcode oder bei Änderungen an der Konfiguration ein neues Maschinenabbild für VirtualBox beziehungsweise ein neues Container-Image für Docker erzeugt werden. Die Dauer für die Erzeugung lässt sich derart der Skala von 1 bis 10 zuordnen, dass die langsamere der beiden Erzeugungsvorgänge die Bewertung 1 erhält und die anderen einen entsprechend proportionalen Wert, wenn man davon ausgeht, dass eine Erzeugung die gar keine Zeit kostet mit 10 Punkte bewertet wird.

Als zweites Kriterium wird die Dauer für das Starten der ersten Umgebung herangezogen, wobei hier ebenfalls der längere Startvorgang mit 1 und ein sofortiges Starten mit 10 bewertet wird. Hier wird bewusst das Starten der ersten Umgebung gemessen, da es sein könnte, dass mit jeder weiteren Umgebgung, die auf dem gleichen Hardware-Host gestartet wird, der Startvorgang potentiell langsamer wird, da sich die einzelnen Umgebungen ja letzendlich die gleiche Hardwareresourcen teilen.

Der zuletzt angedeutete Aspekt, nämlich ein geändertes Laufzeitverhalten aufgrund von endlichen Resourcen auf einer Hardwaremaschine, soll nun ebenfalls durch ein Kriterium messbar gemacht werden. Wie bereits in der Einleitung erwähnt, ist laut Hamble und Farley bei der Parallelieserung von Tests die Gesamtausführungs aller Tests lediglich durch die Dauer des längsten Tests und die Größe des Hardware-Budgets, also durch die Menge der zur Verfügung stehenden Hardware begrenzt \citep[Vgl.][S. 310]{HumFar10}. In Bezug auf die beiden unterschiedliche Ansätze kann man somit sagen, dass eine Lösung, die weniger Resourcen verbraucht, besser ist als diejenige, die mehr Resourcen verbraucht, da man mit ihr auf der gleichen zur Verfügung stehenden Hardware mehr Testumgebungen und somit mehr Tests parallelisieren kann. Zur Messung dieser Eigenschaft wird deshalb untersucht, wieviele Umgebungen sich mit dem jeweiligen Ansatz auf einer konkreten Testhardware starten lassen. Die größere Anzahl wird dann wiederrum mit dem Wert 10 bewertet, die kleinere entsprechend proportional, wobei 1 für nur eine gestartete Umgebung steht. Es wäre noch spannender, sogar das tatsächliche Laufzeitverhalten einer gestarteten Umgebung in Bezug auf die zur Verfügung stehenden Hardware-Resourcen und der Anzahl an anderen Testumgebungen zu untersuchen. So kann es zwar sein, dass beide Ansätze gleich viele Umgebungen starten können, diese aber langsamer oder schneller Anwendungsanfragen abarbeiten. Erste Versuche, entsprechende Zahlen zu ermitteln haben sich aber als sehr schwierig herausgestellt, da gerade im Grenzbereich (also zum Beispiel komplett ausgenutzter Hardwareresourcen) sehr heterogene und schwer reproduzierbare Werte entstehen. Im Rahmen dieser Evaluation wird eine solche Messung also nicht vorgenommen.

Alle drei genannten Kriterien, Erzeugungsdauer, Startdauer und Menge an Umgebungen pro konkreter Hardwareresource sind für die Problemstellung und Ausgangssituation bei der Pixelhouse GmbH von großer Bedeutung. Da aber grundsätzlich ja eine größtmögliche Parallelisierung der Testausführung angestrebt wird, ist die Menge der Umgebungen das wichtigste Kriterium. Es wird somit mit dem Faktor 10 gewichtet. Da sich auch die Erzeugungsdauer der Maschinenabbilder maximal auf soviele Prozesse parallelisieren lässt, wie es konkrete Komponenten innerhalb der Umgebung gibt, spielt auch diese Größe eine wichtige Rolle. Sie wird etwas schwächer aber immer noch mit 7 Punkten gewichtet. Die Startdauer einer einzelnen Umgebung spielt eine geringere Rolle, da sie, wenn man von einer maximalen Parallelisierung ausgehen würde, die Gesamtausführungsdauer aller Tests ja nur konstant erhöhen würde und nicht zum Beispiel linear oder sogar exponentiell. Dauert das Starten zum Beispiel 10 Minuten, würde die Gesamtausführungsdauer aller Tests sich eben um 10 minuten erhöhen, wenn man alle Tests parallel ausführt. Dieses Kriterium wird deshalb lediglich mit dem Faktor 3 gewichtet.

Neben Kriterien, die Einfluss auf die Ausführungsdauer haben, spielen für die Pixelhouse nun lediglich noch Kriterien eine Rolle, die die Verwendbarkeit des jeweiligen Ansatzes kennzeichnen. Hier soll jeweils die bessere Lösung mit dem Wert 10 bewertet werden, die schlechtere Lösung mit 1. Die konkreten Kriterien sind dabei: Installationsaufwand, Einfachheit der Konfigurationssyntax (für Maschinen und Umgebungsdefinition) und Einfachheit der Umgebungssteuerung (Kommandozeilentools).

\subsubsection{Ermittlung und Diskussion der Ergebnisse}
