\section{Zusammenfassung und Ausblick}

Im Rahmen dieser Arbeit wurde zunächst ein Überblick über die beiden Themenbereiche Softwaretests und Virtualsierung und deren aktuellen Stand der Forschung gegeben.

Im Bereich der Softwaretests wurden dabei die unterschiedlichen Testarten Unit-, Integrations-, System- und Akzeptanz-Tests erläutert und festgestellt, dass hier vor allem die System- und Akzeptanz-Tests hohe Anforderungen an ihre Testumgebung und die Ausführungsdauer von Tests stellen. Außerdem wurden verschiedene Strategien zur Testausführung besprochen und argumentiert, warum es sinnvoll ist, sich für eine automatisierte Auführung der Tests zu entscheiden und welche Testarten hierbei mehr oder weniger intensiv verwendet werden sollten.

Im Bereich der Virtualisierung wurde zunächst eine grundsätzliche Kategorisierung zwischen der Systemvirtualisierung mittels Hypervisor und der Betriebssystemvirtualisierung mittels OS-Containern
erarbeitet. Dabei wurden verschiedene Produkte und technische Lösungen beschrieben, ihre Vor- und Nachteile erörtert und abschließend vergleichend diskutiert. Hierbei war die entscheidenste Erkenntnis, dass OS-Container-Lösungen grundsätzlich resourceneffizienter sind. So muss für das Starten eines Containers kein neues Betriebssystem gestartet werden, wie das bei einer vollwertigen virtuellen Maschine auf Basis eines Hypervisors der Fall ist. Lediglich die in ihm enthaltenen Anwendungsprogramme müssen gestartet werden. Auch der Festplattenverbrauch ist aufgrund der Nutzung des sogenannten UnionFilesystems wesentlich effizienter als bei den anderen Lösungen. Als einzige nicht in Frage kommende Lösung wurde der Unikernel auf Basis von MirageOS identifiziert.

Im darauf folgenden Kapitel "`Konzeption"' wurden zunächst die aktuelle Produktiv- und die aktuellen Testumgebungen der Pixelhouse GmbH beschrieben und ihre Nachteile erarbeitet. Zu diesen Nachteilen gehören vor allem die Tatsache, dass sie bisher weder Produktiv- noch Testsysteme automatisch aufstezen lassen, da diese aktuell vollständig händisch installiert und konfiguriert werden. Außerdem stößt die Pixelhouse GmbH bei dem Versuch, die Ausführung der Tests zu parallelisieren immer wieder auf das Problem fehlender Test Isolation. Anhand dieser Nachteile und mit Hilfe der im Grundlagen-Kapitel erarbeiteten Möglichkeiten der Virtualisierung wurde dann ein Konzept für eine neue Testumgebung definiert. Hierbei wurde argumentiert, dass die aktuellen Probleme der Pixelhouse GmbH sich dadurch lösen lassen, dass mit Hilfe der Virtualisierung jede Testumgebung ein vollständiges Abbild der Produktivumgebung ist. Dadurch wird zum einen verhindert, dass verschiedene Tests sich beim parallellen Ausführen gegenseitig beeinträchtigen und zum anderen eröffnet es die Möglichkeit, zukünfig auch Änderungen an der Infrastruktur selbst zu testen. Die Definition der Komponenten und der Gesamtumgebung sollen dabei grundsätzlich unter Versionskontrolle stellbar sein.

Im anschließenden Kapitel "`Implementierung"' wurden zwei prototypische Umsetzungen beschrieben. Zum einen wurde eine Implementierung mit Hilfe des Hosted-Hypervisors VirtualBox von Oracle vorgenommen. Dabei kamen vor allem das Programm Packer zum Erzeugen der Maschinen-Abbilder und das Programm Vagrant zum Starten der Gesamtumgebung zum Einsatz. Weiterhin wurde eine Implementierung unter Verwendung der OS-Container-Virtualisierung im Linux-Kernel vorgestellt. Dabei kam zum einen das Tool Docker zum Einsatz, mit dem die einzelnen Images erzeugt und später als Container zur Ausführung gebracht werden. Außerdem wurde das Program docker-compose verwendet, mit dem sich mehrere Container auf Basis von Docker zu einer Gesamtumgebung orchestrieren lassen. Bei beiden Lösungen wurden Probleme beziehungsweise Besonderheiten der jeweiligen Implementierung aufgezeigt.

Im Kapitel "`Evaluation"' wurde zunächst eine Evaluationsmethode definiert, die es ermöglicht, die beiden Lösungen in Hinblick auf die vorliegende Problemstellung und Ausgangslage bei der Pixelhouse GmbH zu bewerten. Dazu wurden zunächst bestimmte Evaluationskriterien formuliert und beschrieben, wie sie sich ermitteln lassen. Außerdem wurde eine Gewichtung vorgenommen, da nicht jedes Kriterium für die Pixelhouse GmbH von gleicher Bedeutung ist. Anschließend wurde beschrieben, wie die jeweiligen Kriterien ermittelt wurden und die tatsächlichen Messwerte zu einem gewichteten Mittelwert verrechnet. Dabei ergab sich eine deutliche Tendenz für die Lösung mit Hilfe der OS-Container-Virtualisierung und dem Produkt Docker. Dies lag vor allem an einem deutlich schnelleren Erzeugen und Starten der Umgebungen und daran, dass mit Docker wesentlich mehr Umgebungen auf dem selben Testsystem parallel betrieben werden konnten als mit VirtualBox.

Dieses Ergebnis passt positiverweise zu der Strategie der System-Administratoren der Pixelhouse GmbH, die bereits daran arbeiten, die Komponenten des Produktivsystems auf eine OS-Container-Virtualisierung mit Hilfe von Docker umzustellen. Deren Herausforderung ist dabei aktuell, dass aufgrund der hohen Last auf diesem System mehr als ein phsysischer Server verwendet werden muss. Sowohl Docker als auch docker-compose arbeiten grundsätzlich nur auf einem Host. Für die Verwaltung von Testsystemen im Sinne dieser Arbeit ist dies vollkommen ausreichend. Im Produktivsystem sind für die Orchestrierung von Containern über mehrere Server hinweg zusätzliche Werkzeuge notwendig. Dies könnte ein interessantes Thema für eine weitere wissenschaftliche Betrachtung sein.

Eine andere interessante Fragestellung ist, inwieweit sich die erarbeiteten Ansätze auch für die Nutzung in der lokalen Entwicklungsumgebung auf dem Laptop eines Entwicklers eignen. Dazu muss vor allem geklärt werden, wie der Entwickler mit seiner IDE effizient auf den Programmcode innerhalb der virtuellen Maschine oder des OS-Containers zugreifen kann. Auch dies wird eine der zukünftigen Herausforderungen für die Pixelhouse GmbH sein.

Denn schlussendlich ist es der Wunsch der Pixelhouse GmbH, dass in allen drei Umgebungen, also Entwicklungsumgebung, Testumgebung und Produktivumgebung, die gleichen Komponenten zum Einsatz kommen und sowenig wie möglich voneinander abweichen, um dadurch fehlerhafte Implementierungen und falsche Testaussagen aufgrund abweichender Umgebungen zu minimieren.